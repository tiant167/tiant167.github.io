---
layout: post
title:  AIGC 内容良品率的思考与措施
date:   2025-11-30 20:10:14 +0800
---
{% include aligner.html images="post-images/aigc-1.webp" %}

在最近的项目中，我们正在解决 **营销文案 AIGC 的良品率** 问题。我们的业务场景是用大模型生成投放用的营销内容，例如短信、朋友圈广告等，并将这一能力嵌入到一个完整的 **自动化 AI 工作流** 中：前有投放人群拆分，后有自动化配置，全流程在配置完成之前几乎 **不经过人工介入**。

这带来一个非常关键的要求：**文案生成必须一次成型**。系统每次只会生成一条文案，而这一条就会直接进入后续配置流程。虽然在最终投放前仍会有人工的复核与微调，但如果修改过多，就失去了自动化与智能化的意义。因此，提高生成内容的良品率是我们必须解决的核心问题。

除了生成次数的限制，**内容精准性** 也是巨大挑战。投放文案通常包含复杂的营销权益信息，而这些权益本身往往存在不确定性，因此文案中必须加入大量模糊性表达，例如“有机会”“最低 X 折”等等。与此同时，所有非权益类表达都必须 **极度严谨**，不能出现任何承诺、暗示或诱导。

但矛盾在于，业务又期待 AIGC 文案具备 **多样性与创意**。僵硬、机械的表达无法提升点击率，也无法胜过人工写作的效果。因此，我们既要确保文案合规、准确，又要保证语言灵活、多样，并且能带来更好的投放表现。

在面对不够高的良品率问题时，让我开始思考良品率低的核心问题是什么？

我觉得核心问题来自于大模型指令遵循的本质是“基于统计概率的序列预测”，而不是“逻辑执行”

大模型并不是在 “理解” 并 “执行” 提示词指令，而是根据从海量数据中学到的统计规律，生成一段最类似于“遵循了此类指令的文本”。大模型的目标是生成一个在统计概率上最“合理”、最“像”正确答案的文本序列。

这会导致两个致命的问题：

1. 约束条件容易被“淹没”或“降权”：当指令非常长、约束非常多时，模型可能会优先满足它认为**更“重要”的模式**（比如“写一条短信”的宏观模式），而忽略掉一些细节的约束（比如“需要添加‘有机会’的约束”）。因为在 Transformer 的注意力机制中，长指令会导致一些 token 的重要性在注意力分配中权重很低，尤其低频规则容易被忽略
2. 缺乏真正的推理链：这里所说的真正的推理链指的是人类世界的逻辑系统，基于明确的规则进行演绎，每一步都是从前一步必然地推导出来的。而提示词中的 COT 或 Thinking 模式下的思考过程，本质仍然是在遵循统计学上的文本模式，模型在执行 COT 时是在**模仿推理的形式，而不是在进行必然性推导**
    1. 举个例子，COT 中说 “第三步：检查字数小于 50 字”，模型在这里可能会输出 “我现在执行第三步，检查了字数，符合要求”。实际上，他只是根据前面的文字和训练时见过的无数语料，预测这句话应该出现，而不是真的在检查字数。
    2. 那 COT 为什么仍然会有效果？原因是 COT 强迫模型放慢了速度，迫使模型将一个大问题分解成多个子问题，逐个处理，避免了直接一步跳到最终答案时可能犯的错误。COT 的作用时让模型在生成时注意更多中间环节，但它并不能保证每一步是真实计算。

除了核心问题之外，其他一些问题同样会导致良品率不高：

1. 长上下文导致“注意力稀释”
2. “隐含知识”与“常识”的缺失
3. 创造力与规则性的内在冲突

种种问题加在一起会导致**单次推理的良品有极限**，在使用 ChatGPT、Claude 等模型时的良品率主观感受在 50%-60% 。那么，如果我要实现一个良品率在 70% 以上，甚至达到 90% 的内容生成模型应该怎么做？

1. 首先是提示词的优化：我对原始的提示词进行了 COT 指令的加强，让模型花更多的时间在推理上。接着是规则的精细化，使用代码逻辑对约束项进行筛选和组装，仅把和当前场景相关的约束放入提示词中。例如：发放打折权益时，其他类型的权益约束都不需要写入提示词。
2. 大参数量模型升级：当前使用的 qwen3-32b 经过观察很容易跳过 COT 一步出结果，并且内容生成质量也不尽如人意。通过升级到 qwen3-235b 对于整体质量有很大提升。指令遵循、背景信息的理解都比较稳定。
3. 生成-审核-改写循环：这一步是在生成节点之后加入一个审核节点，包含大模型审核、正则表达式审核和长度计算审核。审核不通过会输出原因和改进建议给到下一个改写模型。通过三步来完成一次内容生成，对于常见的权益约束失败、长度超长等问题有很好的效果。
4. 主子 Agent 规划模式：上面说的“三步走”方案再推进一步，便是主子 Agent 规划模式，通过一个主 Agent 去分解生成步骤、拆分不同任务给到对应的子 Agent，通过分工协作实现更长的推理耗时，以此来提升良品率。
5. 最后是模型的微调，但介于显卡的紧缺程度，目前还没有实际使用。但通过线上反馈数据进行 SFT 或 DPO 应该都是非常有效提升良品和点击率的方法。我们会在今年逐步尝试并对比效果。

以上是我们在提升内容生成良品率方面的一些思考与实践。但良品率并不是一次性的工程，它会随着业务策略、投放规则甚至市场环境的变化而不断波动。过去表现良好的方法也可能在新的条件下失效。因此，提高良品率需要持续的关注、迭代与投入，只有把这件事当成长期能力建设，才能真正让 AIGC 在实际业务中稳定创造价值。
